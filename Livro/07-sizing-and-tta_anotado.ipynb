{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"jupytext":{"split_at_heading":true},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#hide\n! [ -e /content ] && pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:22:31.531863Z","iopub.execute_input":"2025-07-02T14:22:31.532058Z","iopub.status.idle":"2025-07-02T14:24:06.542865Z","shell.execute_reply.started":"2025-07-02T14:22:31.532030Z","shell.execute_reply":"2025-07-02T14:24:06.542119Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.1/124.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#hide\nfrom fastbook import *","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:41:55.983124Z","iopub.execute_input":"2025-07-02T14:41:55.983484Z","iopub.status.idle":"2025-07-02T14:41:55.988929Z","shell.execute_reply.started":"2025-07-02T14:41:55.983456Z","shell.execute_reply":"2025-07-02T14:41:55.988053Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Training a State-of-the-Art Model","metadata":{}},{"cell_type":"markdown","source":"## Imagenette","metadata":{}},{"cell_type":"code","source":"from fastai.vision.all import *\npath = untar_data(URLs.IMAGENETTE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:41:58.176500Z","iopub.execute_input":"2025-07-02T14:41:58.176847Z","iopub.status.idle":"2025-07-02T14:42:36.957712Z","shell.execute_reply.started":"2025-07-02T14:41:58.176820Z","shell.execute_reply":"2025-07-02T14:42:36.956785Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='1557168128' class='' max='1557161267' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      100.00% [1557168128/1557161267 00:27&lt;00:00]\n    </div>\n    "},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\ndls = dblock.dataloaders(path, bs=64)\n\n# Datablock semelhante ao abordado em cadernos anteriores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:43:47.961551Z","iopub.execute_input":"2025-07-02T14:43:47.961898Z","iopub.status.idle":"2025-07-02T14:43:49.485434Z","shell.execute_reply.started":"2025-07-02T14:43:47.961873Z","shell.execute_reply":"2025-07-02T14:43:49.484510Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"model = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n# xresnet50: é uma função que retorna uma rede neural com arquitetura pré-definida, mas sem pré treinamento.\n# n_out=dls.c: define o número de saídas (classes) do modelo.\n# dls.c pega automaticamente quantas categorias diferentes existem no conjunto de dados (por exemplo, 10 se for classificação de 10 tipos de imagens).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:43:53.117961Z","iopub.execute_input":"2025-07-02T14:43:53.118265Z","iopub.status.idle":"2025-07-02T14:56:28.473611Z","shell.execute_reply.started":"2025-07-02T14:43:53.118241Z","shell.execute_reply":"2025-07-02T14:56:28.472471Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.590252</td>\n      <td>1.672649</td>\n      <td>0.463032</td>\n      <td>02:24</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.197253</td>\n      <td>1.363934</td>\n      <td>0.560866</td>\n      <td>02:32</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.961066</td>\n      <td>1.037857</td>\n      <td>0.661688</td>\n      <td>02:32</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.734662</td>\n      <td>0.658592</td>\n      <td>0.794249</td>\n      <td>02:33</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.609249</td>\n      <td>0.545648</td>\n      <td>0.830097</td>\n      <td>02:32</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"## Normalization","metadata":{}},{"cell_type":"code","source":"x,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])\n\n# \"Para cada canal de cor, calcule a média e dp de todos os pixels em todas as imagens.\"\n# Ele pega um batch (lote) de dados do DataLoader.\n# x: são as imagens (normalmente um tensor com shape [bs, 3, h, w] = lote, canais, altura, largura)\n\n# O que faz x.mean(dim=[0,2,3])?\n# Isso calcula a média dos pixels das imagens:\n# dim=0: percorre todos os itens do batch\n# dim=2,3: percorre altura e largura da imagem (pixels)\n# Deixa de fora dim=1, que representa os 3 canais de cor: vermelho, verde e azul (RGB).\n\n# Resultado: você obtém uma média por canal de cor: [Média dos valores do canal vermelho, Média dos valores do canal verde, Média dos valores do canal azul]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:57:06.467316Z","iopub.execute_input":"2025-07-02T14:57:06.468238Z","iopub.status.idle":"2025-07-02T14:57:07.309210Z","shell.execute_reply.started":"2025-07-02T14:57:06.468199Z","shell.execute_reply":"2025-07-02T14:57:07.308488Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(TensorImage([0.4675, 0.4609, 0.4204], device='cuda:0'),\n TensorImage([0.2911, 0.2801, 0.2950], device='cuda:0'))"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def get_dls(bs, size):\n    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n                   get_items=get_image_files,\n                   get_y=parent_label,\n                   item_tfms=Resize(460),\n                   batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])\n    return dblock.dataloaders(path, bs=bs)\n\n\n# Esse datablock vira uma função reutilizável, que recebe:\n# bs: tamanho do batch\n# size: tamanho da imagem final usada no treinamento (ex: 224)\n# Isso te permite testar diferentes tamanhos sem reescrever o DataBlock.\n\n# Normalize.from_stats(...)\n# Normaliza canal por canal (R, G, B), usando as médias e desvios padrão do ImageNet\n# *aug_transforms(...)\tDesempacota a lista de transformações para combiná-las com Normalize\n\n# Importante: Quando você usa vision_learner, o fastai já aplica normalização automaticamente, porque ele sabe que você está usando um modelo pré-treinado (como ResNet ou XResNet).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:57:54.855478Z","iopub.execute_input":"2025-07-02T14:57:54.855854Z","iopub.status.idle":"2025-07-02T14:57:54.861767Z","shell.execute_reply.started":"2025-07-02T14:57:54.855828Z","shell.execute_reply":"2025-07-02T14:57:54.860784Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"dls = get_dls(64, 224)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:57:55.451243Z","iopub.execute_input":"2025-07-02T14:57:55.451575Z","iopub.status.idle":"2025-07-02T14:57:56.102886Z","shell.execute_reply.started":"2025-07-02T14:57:55.451549Z","shell.execute_reply":"2025-07-02T14:57:56.101782Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"x,y = dls.one_batch()\nx.mean(dim=[0,2,3]),x.std(dim=[0,2,3])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:58:00.070140Z","iopub.execute_input":"2025-07-02T14:58:00.070468Z","iopub.status.idle":"2025-07-02T14:58:00.888841Z","shell.execute_reply.started":"2025-07-02T14:58:00.070444Z","shell.execute_reply":"2025-07-02T14:58:00.887926Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(TensorImage([-0.0620,  0.0279,  0.1135], device='cuda:0'),\n TensorImage([1.1985, 1.2326, 1.3172], device='cuda:0'))"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T14:58:01.740715Z","iopub.execute_input":"2025-07-02T14:58:01.741049Z","iopub.status.idle":"2025-07-02T15:10:52.178329Z","shell.execute_reply.started":"2025-07-02T14:58:01.741024Z","shell.execute_reply":"2025-07-02T15:10:52.177252Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.633674</td>\n      <td>2.805343</td>\n      <td>0.398058</td>\n      <td>02:35</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.289020</td>\n      <td>2.187535</td>\n      <td>0.471621</td>\n      <td>02:33</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.955947</td>\n      <td>1.016960</td>\n      <td>0.674384</td>\n      <td>02:33</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.740758</td>\n      <td>0.687258</td>\n      <td>0.787528</td>\n      <td>02:33</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.612378</td>\n      <td>0.575446</td>\n      <td>0.815534</td>\n      <td>02:33</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"## Progressive Resizing","metadata":{}},{"cell_type":"code","source":"dls = get_dls(128, 128)\nlearn = Learner(dls, xresnet50(n_out=dls.c), loss_func=CrossEntropyLossFlat(), \n                metrics=accuracy)\nlearn.fit_one_cycle(4, 3e-3)\n\n# dls = get_dls(128, 128) Você está criando um DataLoaders com:\n# Batch size = 128\n# Tamanho das imagens = 128×128\n# Ou seja, as imagens são pequenas no começo → mais rápido de treinar.\n# Treina por 4 épocas com imagens pequenas.\n\n# Treinar imagens pequenas é mais rápido\n# O modelo pode aprender formas básicas e padrões globais\n# Menos custo computacional\n# Serve como um pré-treinamento leve\n# Melhora generalização - é uma especie de data augmentation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:13:29.632810Z","iopub.execute_input":"2025-07-02T15:13:29.633173Z","iopub.status.idle":"2025-07-02T15:18:40.177689Z","shell.execute_reply.started":"2025-07-02T15:13:29.633140Z","shell.execute_reply":"2025-07-02T15:18:40.176866Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.616036</td>\n      <td>3.027645</td>\n      <td>0.391337</td>\n      <td>01:17</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.234733</td>\n      <td>2.027993</td>\n      <td>0.530246</td>\n      <td>01:17</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.957411</td>\n      <td>0.985928</td>\n      <td>0.702390</td>\n      <td>01:17</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.730644</td>\n      <td>0.674533</td>\n      <td>0.794249</td>\n      <td>01:16</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"learn.dls = get_dls(64, 224)\nlearn.fine_tune(5, 1e-3)\n\n# Você substitui os dados do Learner por um novo DataLoaders, agora com:\n# Batch size = 64 (menor, pois imagem maior consome mais memória)\n# Imagem maior = 224×224\n# Isso é o resize progressivo: agora o modelo vai treinar com mais detalhes visuais, pois as imagens têm mais resolução.\n# E como o modelo já aprendeu coisas úteis nas imagens menores, ele agora refina o que sabe com mais detalhes.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:20:07.581786Z","iopub.execute_input":"2025-07-02T15:20:07.582139Z","iopub.status.idle":"2025-07-02T15:35:22.593442Z","shell.execute_reply.started":"2025-07-02T15:20:07.582110Z","shell.execute_reply":"2025-07-02T15:35:22.592529Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.819846</td>\n      <td>1.182295</td>\n      <td>0.664302</td>\n      <td>02:33</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.643167</td>\n      <td>0.800408</td>\n      <td>0.765497</td>\n      <td>02:31</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0.678964</td>\n      <td>0.772617</td>\n      <td>0.757655</td>\n      <td>02:32</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.598801</td>\n      <td>0.522853</td>\n      <td>0.843167</td>\n      <td>02:32</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.489412</td>\n      <td>0.469953</td>\n      <td>0.859597</td>\n      <td>02:32</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.434970</td>\n      <td>0.439171</td>\n      <td>0.864824</td>\n      <td>02:31</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"## Test Time Augmentation","metadata":{}},{"cell_type":"code","source":"preds,targs = learn.tta()\naccuracy(preds, targs).item()\n\n# O que learn.tta() faz?\n# Aplica várias transformações leves (como flip horizontal, zoom, rotação leve, etc.) a cada imagem do conjunto de validação.\n# Faz várias predições por imagem (uma para cada versão transformada).\n# Depois, tira a média das predições de cada imagem.\n# Retorna:\n# preds: tensor com as predições médias finais para cada imagem. \n# targs: rótulos verdadeiros do conjunto de validação.\n\n# Agora, o que accuracy(preds, targs) faz:\n# preds: um tensor com médias das predições softmax para cada imagem ([n_imagens, n_classes])\n# targs: os rótulos verdadeiros, como inteiros ([n_imagens])\n\n# Para cada linha de preds, pega o índice da classe com maior valor, compara com os rótulos verdadeiros (targs), calcula a proporção de acertos.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:37:14.304075Z","iopub.execute_input":"2025-07-02T15:37:14.304360Z","iopub.status.idle":"2025-07-02T15:38:30.860581Z","shell.execute_reply.started":"2025-07-02T15:37:14.304337Z","shell.execute_reply":"2025-07-02T15:38:30.859684Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0.869305431842804"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## Mixup","metadata":{}},{"cell_type":"markdown","source":"### Sidebar: Papers and Math","metadata":{}},{"cell_type":"markdown","source":"### End sidebar","metadata":{}},{"cell_type":"code","source":"church = PILImage.create(get_image_files_sorted(path/'train'/'n03028079')[0])\ngas = PILImage.create(get_image_files_sorted(path/'train'/'n03425413')[0])\nchurch = church.resize((256,256))\ngas = gas.resize((256,256))\ntchurch = tensor(church).float() / 255.\ntgas = tensor(gas).float() / 255.\n\n_,axs = plt.subplots(1, 3, figsize=(12,4))\nshow_image(tchurch, ax=axs[0]);\nshow_image(tgas, ax=axs[1]);\nshow_image((0.3*tchurch + 0.7*tgas), ax=axs[2]);\n\n# Pegando uma imagem de uma igreja (church) e uma de um posto de gasolina (gas)\n# Redimensionando as duas para 256×256\n# Convertendo para tensores e normalizando os valores de pixel para o intervalo [0,1]\n# Mostrando:\n# A imagem da igreja\n# A imagem do posto\n# Uma interpolação das duas (com pesos 0.3 e 0.7)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=MixUp())\nlearn.fit_one_cycle(5, 3e-3)\n\n# ✅ cbs=MixUp()\n# Isso ativa o callback MixUp\n# Durante o treinamento, ele pega pares aleatórios de imagens e rótulos, e faz uma interpolação aleatória entre eles\n# O peso de mistura (ex: 0.3/0.7) é escolhido de uma distribuição Beta (por padrão, Beta(0.4, 0.4))\n\n# O que acontece nos bastidores:\n# As imagens são misturadas: x = λ * x1 + (1 - λ) * x2\n# Os rótulos são misturados: y = λ * y1 + (1 - λ) * y2\n# O modelo é treinado para prever a distribuição entre classes\n# CrossEntropyLossFlat lida automaticamente com esses rótulos mistos","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Label Smoothing","metadata":{}},{"cell_type":"code","source":"model = xresnet50(n_out=dls.c)\nlearn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)\nlearn.fit_one_cycle(5, 3e-3)\n\n# Você está criando um modelo com a função de perda LabelSmoothingCrossEntropy, em vez da tradicional CrossEntropyLossFlat.\n\n# O que é Label Smoothing?\n# É uma técnica onde você não usa rótulos “duros” (one-hot 100%), e sim distribuições suavizadas.\n# Em vez de representar a classe correta como [0, 0, 1, 0, 0]\n# Você representa como [0.01, 0.01, 0.96, 0.01, 0.01] (valores aproximados)\n\n# Por que isso ajuda?\n# Evita que o modelo fique “super confiante” (probabilidade de 100% em uma só classe)\n# Ajuda na regularização (parecido com dropout)\n# Reduz overfitting\n# Pode melhorar a calibração das probabilidades.\n\n# Importante: o benefício dessa técnica só aparece quando o modelo é treinado por muitas épocas. No começo o desempenho pode ser pior\n\n# Por que precisa de mais épocas para funcionar bem?\n# 1. Porque ela freia o aprendizado \"rápido e fácil\"\n# Sem label smoothing, o modelo pode decorar rapidamente os rótulos, especialmente em datasets pequenos ou fáceis\n# Com smoothing, ele precisa pensar mais — o aprendizado inicial é um pouco mais lento, mas tende a ser mais robusto a longo prazo\n\n# 2. Porque o modelo precisa tempo para se adaptar\n# A perda suavizada não dá uma recompensa tão “clara” a cada acerto\n# O modelo leva mais tempo para identificar padrões úteis reais, já que não é punido ou premiado de forma tão extrema\n\n# 3. Porque os benefícios aparecem na generalização, não no overfitting rápido\n# Se você treinar por poucas épocas, pode parecer que o modelo com label smoothing está “pior”\n# Mas com mais treino, ele tende a ter melhor desempenho no conjunto de validação/teste\n\n# O benefício real pode so aparecer depois de 10–20 épocas (dataset pequeno), 15–30 épocas (dataset médio), 30–90+ épocas (dataset grande)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-02T15:40:32.493687Z","iopub.execute_input":"2025-07-02T15:40:32.494052Z","iopub.status.idle":"2025-07-02T15:46:57.013171Z","shell.execute_reply.started":"2025-07-02T15:40:32.494020Z","shell.execute_reply":"2025-07-02T15:46:57.012258Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n    /* Turns off some styling */\n    progress {\n        /* gets rid of default border in Firefox and Opera. */\n        border: none;\n        /* Needs to be in here for Safari polyfill so background images work as expected. */\n        background-size: auto;\n    }\n    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n    }\n    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n        background: #F44336;\n    }\n</style>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.766246</td>\n      <td>2.423440</td>\n      <td>0.457804</td>\n      <td>01:15</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1.525698</td>\n      <td>1.377476</td>\n      <td>0.643764</td>\n      <td>01:16</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.306419</td>\n      <td>1.261292</td>\n      <td>0.690067</td>\n      <td>01:16</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.142434</td>\n      <td>1.228609</td>\n      <td>0.706497</td>\n      <td>01:17</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.035957</td>\n      <td>0.996405</td>\n      <td>0.806946</td>\n      <td>01:17</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"### Sidebar: Label Smoothing, the Paper","metadata":{}},{"cell_type":"markdown","source":"### End sidebar","metadata":{}},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"## Questionnaire","metadata":{}},{"cell_type":"markdown","source":"1. What is the difference between ImageNet and Imagenette? When is it better to experiment on one versus the other?\n1. What is normalization?\n1. Why didn't we have to care about normalization when using a pretrained model?\n1. What is progressive resizing?\n1. Implement progressive resizing in your own project. Did it help?\n1. What is test time augmentation? How do you use it in fastai?\n1. Is using TTA at inference slower or faster than regular inference? Why?\n1. What is Mixup? How do you use it in fastai?\n1. Why does Mixup prevent the model from being too confident?\n1. Why does training with Mixup for five epochs end up worse than training without Mixup?\n1. What is the idea behind label smoothing?\n1. What problems in your data can label smoothing help with?\n1. When using label smoothing with five categories, what is the target associated with the index 1?\n1. What is the first step to take when you want to prototype quick experiments on a new dataset?","metadata":{}},{"cell_type":"markdown","source":"### Further Research\n\n1. Use the fastai documentation to build a function that crops an image to a square in each of the four corners, then implement a TTA method that averages the predictions on a center crop and those four crops. Did it help? Is it better than the TTA method of fastai?\n1. Find the Mixup paper on arXiv and read it. Pick one or two more recent articles introducing variants of Mixup and read them, then try to implement them on your problem.\n1. Find the script training Imagenette using Mixup and use it as an example to build a script for a long training on your own project. Execute it and see if it helps.\n1. Read the sidebar \"Label Smoothing, the Paper\", look at the relevant section of the original paper and see if you can follow it. Don't be afraid to ask for help!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}